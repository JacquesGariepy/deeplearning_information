# deeplearning information
## Information sur le DeepLearning


# 1. Les réseaux de neurones, ou NNs, sont comme des cerveaux miniatures dans l'ordinateur.
# Les NNs sont utilisés pour la reconnaissance vocale pour transformer le son de ta voix en mots compréhensibles.

# 2. L'apprentissage supervisé, ou SL, est comme apprendre avec un professeur.
# SL est la technique utilisée pour la détection de spam. L'ordinateur apprend à partir d'e-mails étiquetés comme "spam" ou "non-spam".

# 3. L'apprentissage non supervisé, ou USL, est comme apprendre seul en explorant.
# USL permet de recommander des produits en regroupant des produits similaires pour faire des suggestions personnalisées.

# 4. L'apprentissage par renforcement, ou RL, est comme apprendre à jouer à un jeu vidéo.
# RL est la technique utilisée pour entraîner des robots. Le robot reçoit des récompenses pour des actions qui l'aident à atteindre son objectif.

# 5. Les réseaux de neurones convolutionnels, ou CNNs, sont comme des yeux pour l'ordinateur.
# Les CNNs sont utilisés pour la reconnaissance d'images. Ils identifient les détails dans l'image pour déterminer ce qu'elle représente.

# 6. Les réseaux de neurones récurrents, ou RNNs, sont comme lire un livre en se rappelant les pages précédentes.
# Les RNNs sont utilisés pour la traduction automatique en utilisant le contexte des mots précédents pour traduire correctement le texte.

# 7. Les autoencodeurs, ou AEs, sont comme dessiner un paysage avec moins de détails, mais toujours reconnaissable.
# Les AEs sont utilisés pour la réduction de dimension. Ils simplifient les données en conservant les informations les plus importantes.

# 8. Le transfert d'apprentissage, ou TL, est comme utiliser tes connaissances de la guitare pour apprendre le ukulélé.
# TL est utilisé dans le traitement de l'image médicale où un modèle formé sur des images générales est adapté pour reconnaître des anomalies spécifiques.

# 9. Le dropout est comme apprendre à jongler en enlevant parfois une balle.
# Le dropout est utilisé pour prévenir le surapprentissage, en permettant au modèle de ne pas dépendre trop fortement d'un seul neurone.

# 10. Le gradient descendant, ou GD, est comme descendre d'une montagne en faisant toujours le pas qui descend le plus.
# GD est utilisé pour optimiser presque tous les modèles d'apprentissage profond en ajustant les poids des neurones pour minimiser l'erreur de prédiction.

# 11. La rétropropagation, ou BP, est comme corriger ses erreurs pour mieux faire la prochaine fois.

# BP est utilisée dans l'entraînement des réseaux de neurones pour ajuster les poids et minimiser l'erreur entre la prédiction et la vérité.

# 12. Le biais, ou bias, est comme un ajustement pour bien calibrer un instrument de musique.

# Bias est utilisé dans tous les neurones des réseaux de neurones pour décaler la sortie du neurone, améliorant la flexibilité du modèle.

# 13. Le surapprentissage, ou overfitting, est comme réviser seulement les questions d'un vieux test pour un nouvel examen.

# Overfitting est un problème évité en séparant les données en ensembles d'entraînement et de test. Si le modèle est trop précis sur l'entraînement, il peut échouer sur le test.

# 14. La régularisation, ou reg, est comme mettre des limites à un enfant pour l'aider à grandir de manière équilibrée.

# Reg est utilisée pour prévenir le surapprentissage en ajoutant une pénalité aux poids élevés pour garder le modèle simple.

# 15. Les réseaux génératifs adverses, ou GANs, sont comme deux artistes qui travaillent ensemble, l'un créant, l'autre critiquant.

# Les GANs sont utilisés pour créer des images réalistes, comme de nouvelles images de visages, d'objets, etc.

# 16. Les réseaux de neurones profonds, ou DNNs, sont comme des cerveaux très complexes dans l'ordinateur.

# Les DNNs sont utilisés pour la reconnaissance vocale. Ils peuvent extraire des fonctionnalités complexes du son pour comprendre le discours.

# 17. Les réseaux résiduels, ou ResNets, sont comme un système de raccourcis pour un trajet long et complexe.

# Les ResNets sont utilisés pour la reconnaissance d'images. Ils permettent d'entraîner des modèles très profonds sans perdre d'informations.

# 18. Les réseaux de neurones à longue mémoire à court terme, ou LSTMs, sont comme se souvenir des détails importants d'une histoire longue.

# Les LSTMs sont utilisés pour la prédiction de séries temporelles. Ils peuvent se souvenir d'informations passées pour prévoir l'avenir.

# 19. La normalisation par lots, ou batch norm, est comme s'assurer que tous les joueurs dans une équipe ont une chance de jouer.

# Batch norm est utilisé dans l'entraînement des réseaux de neurones. Il aide à stabiliser l'apprentissage et réduit le temps d'entraînement.

# 20. Les réseaux attentionnels, ou attention nets, sont comme lire un livre en soulignant les parties importantes.

# Les attention nets sont utilisés pour la compréhension du langage naturel. Ils permettent au modèle de se concentrer sur les parties importantes du texte.


# 21. Les Word Embeddings, ou w2v, transforment les mots en nombres pour que l'ordinateur puisse les comprendre.

# W2v est utilisé dans la compréhension du langage naturel pour convertir les mots en vecteurs numériques pour une meilleure analyse.

# 22. Les réseaux siamois, ou Siamese nets, sont comme des jumeaux qui se comparent constamment.

# Siamese nets sont utilisés pour la vérification de l'identité. Ils peuvent comparer deux images pour déterminer si elles représentent la même personne.

# 23. La validation croisée, ou CV, est comme faire des répétitions pour un spectacle pour s'assurer qu'il sera bon.

# CV est utilisée pour estimer la performance d'un modèle. Elle permet d'évaluer le modèle sur différentes parties des données pour assurer sa robustesse.

# 24. Les réseaux de capsules, ou CapsNets, sont comme des yeux 3D pour l'ordinateur.

# CapsNets sont utilisés pour la reconnaissance d'objets en 3D. Ils peuvent comprendre la structure spatiale d'un objet à partir d'images 2D.

# 25. Le modèle transformer, ou transformer, est comme un super traducteur et résumeur de texte.

# Transformer est utilisé pour comprendre et générer du texte. Il peut répondre à des questions, résumer des textes et même écrire des histoires.

# 26. L'augmentation de données, ou data aug, est comme créer de nouvelles images à partir d'images existantes.

# Data aug est utilisée pour augmenter la taille des ensembles de données. Elle peut créer de nouvelles images par rotation, zoom ou inversion pour améliorer l'apprentissage.

# 27. Les réseaux neuronaux spiking, ou SNNs, sont comme des simulations du cerveau humain.

# Les SNNs sont utilisés dans la recherche pour comprendre comment les neurones dans le cerveau interagissent.

# 28. La distance euclidienne dans les espaces de features est comme mesurer la similarité entre deux objets.

# Elle peut déterminer à quel point deux images ou deux textes sont similaires.

# 29. L'erreur quadratique moyenne, ou MSE, est comme mesurer à quel point on est loin de la cible.

# MSE est utilisée pour mesurer l'erreur d'un modèle. Elle est utilisée pour ajuster les poids des neurones pendant l'entraînement.

# 30. La fonction d'activation ReLU, ou ReLU, est comme une porte qui ne s'ouvre que lorsque quelqu'un frappe assez fort.

# ReLU est utilisée dans la plupart des réseaux de neurones. Elle aide à décider combien d'information doit passer à travers le neurone.

Deep Learning

|-- Réseaux de Neurones (NNs)

|   |-- Réseaux de Neurones Profonds (DNNs)

|   |-- Réseaux de Neurones Convolutionnels (CNNs)

|   |-- Réseaux de Neurones Récurrents (RNNs)

|   |   |-- Réseaux de Neurones à Longue Mémoire à Court Terme (LSTMs)

|   |-- Auto-Encodeurs (AEs)

|   |-- Réseaux Génératifs Adverses (GANs)

|   |-- Réseaux de Capsules (CapsNets)

|   |-- Réseaux Neuronaux Spiking (SNNs)

|

|-- Apprentissage

|   |-- Apprentissage Supervisé (SL)

|   |-- Apprentissage Non Supervisé (USL)

|   |-- Apprentissage par Renforcement (RL)

|   |-- Transfert d'Apprentissage (TL)

|

|-- Techniques d'Optimisation

|   |-- Rétropropagation (BP)

|   |-- Gradient Descendant (GD)

|   |-- Dropout

|   |-- Normalisation par Lots (Batch Norm)

|   |-- Régularisation (Reg)

|   |-- Validation Croisée (CV)

|   |-- Augmentation de Données (Data Aug)

|

|-- Autres concepts

|   |-- Biais (Bias)

|   |-- Surapprentissage (Overfitting)

|   |-- Réseaux Siamois (Siamese Nets)

|   |-- Transformateurs (Transformers)

|   |-- Word Embeddings (W2V)

|   |-- Distance Euclidienne dans les Espaces de Features

|   |-- Erreur Quadratique Moyenne (MSE)

|   |-- Fonction d'Activation ReLU (ReLU)


